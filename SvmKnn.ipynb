{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,BatchNormalization,MaxPool2D\n",
    "from keras.layers import Dropout, Flatten\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Moduli di scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, ParameterGrid, train_test_split, StratifiedShuffleSplit, StratifiedKFold, StratifiedGroupKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, ParameterGrid, train_test_split, StratifiedShuffleSplit\n",
    "import ml_utilities\n",
    "import ml_visualization\n",
    "import ml_utilities\n",
    "import ml_visualization\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "TESTS_PATH = \"./tests/\"\n",
    "INFO_PATH = \"./tests/experiments_info\"\n",
    "DATASET_TEST_PATH = 'DBs/PenDigits/pendigits_te.txt'\n",
    "DATASET_PATH = 'DBs/PenDigits/pendigits_tr.txt'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    feat_count = 16\n",
    "    data_path = DATASET_PATH # Impostare il percorso corretto\n",
    "    patterns, labels = ml_utilities.load_labeled_dataset_from_txt(data_path, feat_count)\n",
    "    test_path = DATASET_TEST_PATH\n",
    "    x_test = ml_utilities.load_unlabeled_dataset_from_txt(test_path, feature_count)\n",
    "    return patterns, labels, x_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def normalize_data(dataset_patterns, test_x):\n",
    "    scaler = MinMaxScaler(feature_range = (0,1))\n",
    "    scaler.fit(dataset_patterns)\n",
    "    x_train_normalized = scaler.fit_transform(dataset_patterns)\n",
    "    x_test_normalized = scaler.fit_transform(test_x)\n",
    "\n",
    "    return x_train_normalized, x_test_normalized"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def feature_extractor(x_grid):\n",
    "    SIZE = x_grid.shape[1]\n",
    "    activation = 'sigmoid'\n",
    "    feature = Sequential()\n",
    "\n",
    "    feature.add(Conv2D(32, 3, activation = activation, padding = 'same', input_shape = (SIZE, SIZE, 1)))\n",
    "    feature.add(BatchNormalization())\n",
    "\n",
    "    feature.add(Conv2D(32, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "    feature.add(BatchNormalization())\n",
    "    feature.add(MaxPool2D())\n",
    "\n",
    "    feature.add(Flatten())\n",
    "\n",
    "    return feature"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "model_params = {\n",
    "    'svm': {\n",
    "        'model': svm.SVC(gamma='auto'),\n",
    "        'params' : {\n",
    "            'C': [10**x for x in np.arange(-1,3,0.0125)],  #Regularization parameter. Providing only two as SVM is slow\n",
    "            'kernel': ['rbf','linear']\n",
    "        }\n",
    "    },\n",
    "    'SVC': {\n",
    "        'model': SVC(),\n",
    "        'params' : {\n",
    "            'C': [10**x for x in np.arange(-1,3,0.0125)],  #Regularization parameter. Providing only two as SVM is slow\n",
    "            'kernel': ['rbf','linear'],\n",
    "            'gamma': [10**x for x in np.arange(-3,1,0.0125)],\n",
    "            'class_weight':['balanced', None],\n",
    "            'degree' : [1,2,3,4,5,6]\n",
    "        }\n",
    "    },\n",
    "    'KNeighbors': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params' : {\n",
    "            'n_neighbors': [1,3,5,7,9,11,13,15,17,19,21,23,25,27],\n",
    "            'weights': ['uniform','distance'],\n",
    "            'algorithm': [\"ball_tree\",\"kd_tree\",\"brute\"],\n",
    "            'metric': [\"minkowski\",\"euclidean\",\"l1\", \"l2\",\"manhattan\"],\n",
    "\n",
    "        }\n",
    "    },\n",
    "    'logistic_regression' : {\n",
    "        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n",
    "        'params': {\n",
    "            'C': [10**x for x in np.arange(-1,3,0.0125)],  #Regularization. . Providing only two as LR can be slow\n",
    "            'penalty': ['l1', 'l2']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "scores=[]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape dataset: (442, 16)\n",
      "Shape labels: (442,)\n"
     ]
    }
   ],
   "source": [
    "# Caricamento del dataset\n",
    "feature_count = 16\n",
    "dataset_path = 'DBs/PenDigits/pendigits_tr.txt'  # Impostare il percorso corretto\n",
    "\n",
    "dataset_patterns, dataset_labels = ml_utilities.load_labeled_dataset_from_txt(dataset_path, feature_count)\n",
    "print('Shape dataset:', dataset_patterns.shape)\n",
    "print('Shape labels:', dataset_labels.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\.conda\\envs\\Test\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "2 fits failed out of a total of 2.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\David\\.conda\\envs\\Test\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\David\\.conda\\envs\\Test\\lib\\site-packages\\xgboost\\core.py\", line 506, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\David\\.conda\\envs\\Test\\lib\\site-packages\\xgboost\\sklearn.py\", line 1250, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\David\\.conda\\envs\\Test\\lib\\site-packages\\xgboost\\training.py\", line 188, in train\n",
      "    bst = _train_internal(params, dtrain,\n",
      "  File \"C:\\Users\\David\\.conda\\envs\\Test\\lib\\site-packages\\xgboost\\training.py\", line 81, in _train_internal\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"C:\\Users\\David\\.conda\\envs\\Test\\lib\\site-packages\\xgboost\\core.py\", line 1680, in update\n",
      "    _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n",
      "  File \"C:\\Users\\David\\.conda\\envs\\Test\\lib\\site-packages\\xgboost\\core.py\", line 218, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [12:37:58] c:\\windows\\temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\common\\common.h:157: XGBoost version not compiled with GPU support.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\David\\.conda\\envs\\Test\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\David\\.conda\\envs\\Test\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[12:37:58] c:\\windows\\temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\common\\common.h:157: XGBoost version not compiled with GPU support.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mXGBoostError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [48]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model_name, mp \u001B[38;5;129;01min\u001B[39;00m model_params\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;66;03m# pipe = Pipeline([('classifier' , mp['model'])])\u001B[39;00m\n\u001B[0;32m     19\u001B[0m     grid \u001B[38;5;241m=\u001B[39m  RandomizedSearchCV(xgb,\n\u001B[0;32m     20\u001B[0m                                param_distributions\u001B[38;5;241m=\u001B[39mparams,\n\u001B[0;32m     21\u001B[0m                                cv\u001B[38;5;241m=\u001B[39mcross_val,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     24\u001B[0m                                return_train_score\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     25\u001B[0m                                random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m---> 27\u001B[0m     \u001B[43mgrid\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m     scores\u001B[38;5;241m.\u001B[39mappend({\n\u001B[0;32m     29\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata_test_size\u001B[39m\u001B[38;5;124m'\u001B[39m: data_test_size,\n\u001B[0;32m     30\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfolds\u001B[39m\u001B[38;5;124m'\u001B[39m: folds,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     34\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbest_params\u001B[39m\u001B[38;5;124m'\u001B[39m: grid\u001B[38;5;241m.\u001B[39mbest_params_,\n\u001B[0;32m     35\u001B[0m     })\n\u001B[0;32m     37\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(scores,columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata_test_size\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfolds\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbest_score\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean_test_score\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbest_params\u001B[39m\u001B[38;5;124m'\u001B[39m,])\n",
      "File \u001B[1;32m~\\.conda\\envs\\Test\\lib\\site-packages\\sklearn\\model_selection\\_search.py:926\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[0;32m    924\u001B[0m refit_start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m    925\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 926\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_estimator_\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m    927\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    928\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_estimator_\u001B[38;5;241m.\u001B[39mfit(X, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n",
      "File \u001B[1;32m~\\.conda\\envs\\Test\\lib\\site-packages\\xgboost\\core.py:506\u001B[0m, in \u001B[0;36m_deprecate_positional_args.<locals>.inner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    504\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters, args):\n\u001B[0;32m    505\u001B[0m     kwargs[k] \u001B[38;5;241m=\u001B[39m arg\n\u001B[1;32m--> 506\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\Test\\lib\\site-packages\\xgboost\\sklearn.py:1250\u001B[0m, in \u001B[0;36mXGBClassifier.fit\u001B[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001B[0m\n\u001B[0;32m   1230\u001B[0m model, feval, params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_configure_fit(xgb_model, eval_metric, params)\n\u001B[0;32m   1231\u001B[0m train_dmatrix, evals \u001B[38;5;241m=\u001B[39m _wrap_evaluation_matrices(\n\u001B[0;32m   1232\u001B[0m     missing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmissing,\n\u001B[0;32m   1233\u001B[0m     X\u001B[38;5;241m=\u001B[39mX,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1247\u001B[0m     label_transform\u001B[38;5;241m=\u001B[39mlabel_transform,\n\u001B[0;32m   1248\u001B[0m )\n\u001B[1;32m-> 1250\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_Booster \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1251\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dmatrix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1253\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_num_boosting_rounds\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1256\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevals_result\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevals_result\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1258\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfeval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1259\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1260\u001B[0m \u001B[43m    \u001B[49m\u001B[43mxgb_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1261\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1262\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1264\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m callable(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective):\n\u001B[0;32m   1265\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective \u001B[38;5;241m=\u001B[39m params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobjective\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\.conda\\envs\\Test\\lib\\site-packages\\xgboost\\training.py:188\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(params, dtrain, num_boost_round\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, evals\u001B[38;5;241m=\u001B[39m(), obj\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, feval\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    116\u001B[0m           maximize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, early_stopping_rounds\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, evals_result\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    117\u001B[0m           verbose_eval\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, xgb_model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, callbacks\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    118\u001B[0m     \u001B[38;5;66;03m# pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init\u001B[39;00m\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;124;03m\"\"\"Train a booster with given parameters.\u001B[39;00m\n\u001B[0;32m    120\u001B[0m \n\u001B[0;32m    121\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;124;03m    Booster : a trained booster model\u001B[39;00m\n\u001B[0;32m    187\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 188\u001B[0m     bst \u001B[38;5;241m=\u001B[39m \u001B[43m_train_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    189\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mnum_boost_round\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_boost_round\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    190\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mevals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    191\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    192\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mxgb_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mxgb_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mverbose_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    194\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mevals_result\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevals_result\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    195\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    196\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    197\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m bst\n",
      "File \u001B[1;32m~\\.conda\\envs\\Test\\lib\\site-packages\\xgboost\\training.py:81\u001B[0m, in \u001B[0;36m_train_internal\u001B[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m callbacks\u001B[38;5;241m.\u001B[39mbefore_iteration(bst, i, dtrain, evals):\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m---> 81\u001B[0m \u001B[43mbst\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m callbacks\u001B[38;5;241m.\u001B[39mafter_iteration(bst, i, dtrain, evals):\n\u001B[0;32m     83\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\Test\\lib\\site-packages\\xgboost\\core.py:1680\u001B[0m, in \u001B[0;36mBooster.update\u001B[1;34m(self, dtrain, iteration, fobj)\u001B[0m\n\u001B[0;32m   1677\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_features(dtrain)\n\u001B[0;32m   1679\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1680\u001B[0m     \u001B[43m_check_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_LIB\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mXGBoosterUpdateOneIter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1681\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mc_int\u001B[49m\u001B[43m(\u001B[49m\u001B[43miteration\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1682\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mdtrain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1683\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1684\u001B[0m     pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(dtrain, output_margin\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\Test\\lib\\site-packages\\xgboost\\core.py:218\u001B[0m, in \u001B[0;36m_check_call\u001B[1;34m(ret)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;124;03m\"\"\"Check the return value of C API call\u001B[39;00m\n\u001B[0;32m    208\u001B[0m \n\u001B[0;32m    209\u001B[0m \u001B[38;5;124;03mThis function will raise exception when error occurs.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;124;03m    return value from API calls\u001B[39;00m\n\u001B[0;32m    216\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 218\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m XGBoostError(py_str(_LIB\u001B[38;5;241m.\u001B[39mXGBGetLastError()))\n",
      "\u001B[1;31mXGBoostError\u001B[0m: [12:37:58] c:\\windows\\temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\common\\common.h:157: XGBoost version not compiled with GPU support."
     ]
    }
   ],
   "source": [
    "!conda create -n xgboost_env -c nvidia -c rapidsai py-xgboost cudatoolkit=10.2\n",
    "np.random.seed(42)\n",
    "\n",
    "x_train, y_train, x_test = read_data()\n",
    "x_grid, x_not_use, y_grid, y_not_use = train_test_split(x_train,y_train, test_size=0.2,random_state=42)\n",
    "for data_test_size in [0.2]:\n",
    "    for folds in [1]:\n",
    "        cross_val = StratifiedShuffleSplit(n_splits=folds, test_size=data_test_size, random_state=42)\n",
    "        xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic', nthread=1,tree_method='gpu_hist')\n",
    "        params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 5, 7, 10],\n",
    "        'learning_rate': [0.01, 0.02, 0.05]\n",
    "        }\n",
    "        for model_name, mp in model_params.items():\n",
    "            # pipe = Pipeline([('classifier' , mp['model'])])\n",
    "            grid =  RandomizedSearchCV(xgb,\n",
    "                                       param_distributions=params,\n",
    "                                       cv=cross_val,\n",
    "                                       n_jobs=16,\n",
    "                                       n_iter=2,\n",
    "                                       return_train_score=False,\n",
    "                                       random_state=42)\n",
    "\n",
    "            grid.fit(x_train,y_train)\n",
    "            scores.append({\n",
    "                'data_test_size': data_test_size,\n",
    "                'folds': folds,\n",
    "                'model': model_name,\n",
    "                'best_score': grid.best_score_,\n",
    "                'mean_test_score': grid.cv_results_['mean_test_score'],\n",
    "                'best_params': grid.best_params_,\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(scores,columns=['data_test_size','folds','model','best_score','mean_test_score','best_params',])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_test_size= 0.1 and folds = 1\n",
      "fit completed\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "x_train, y_train, x_test = read_data()\n",
    "x_grid, x_not_use, y_grid, y_not_use = train_test_split(x_train,y_train, test_size=0.2,random_state=42)\n",
    "for data_test_size in np.arange(0.1, 1.0, 0.05):\n",
    "    for folds in  np.arange(1, 10, 1):\n",
    "        cross_val = StratifiedShuffleSplit(n_splits=folds, test_size=data_test_size, random_state=42)\n",
    "        for model_name, mp in model_params.items():\n",
    "            grid =  RandomizedSearchCV(estimator=mp['model'],\n",
    "                                       param_distributions=mp['params'],\n",
    "                                       cv=cross_val,\n",
    "                                       n_jobs=16,\n",
    "                                       n_iter=2,\n",
    "                                       return_train_score=False,\n",
    "                                       random_state=42)\n",
    "\n",
    "            grid.fit(x_train,y_train)\n",
    "            scores.append({\n",
    "                'data_test_size': data_test_size,\n",
    "                'folds': folds,\n",
    "                'model': model_name,\n",
    "                'best_score': grid.best_score_,\n",
    "                'mean_test_score': grid.cv_results_['mean_test_score'],\n",
    "                'best_params': grid.best_params_,\n",
    "            })\n",
    "        df = pd.DataFrame(scores,columns=['data_test_size','folds','model','best_score','mean_test_score','best_params',])\n",
    "        print(f\"data_test_size= {data_test_size} and folds = {folds}\")\n",
    "        print(f\"fit completed\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
